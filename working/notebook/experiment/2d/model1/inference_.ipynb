{"cells":[{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2023-04-04T04:51:15.411176Z","iopub.status.busy":"2023-04-04T04:51:15.41084Z","iopub.status.idle":"2023-04-04T04:51:20.141027Z","shell.execute_reply":"2023-04-04T04:51:20.13956Z","shell.execute_reply.started":"2023-04-04T04:51:15.411146Z"},"trusted":true},"outputs":[],"source":["import sys\n","sys.path.append('/kaggle/input/pretrainedmodels/pretrainedmodels-0.7.4')\n","sys.path.append('/kaggle/input/efficientnet-pytorch/EfficientNet-PyTorch-master')\n","sys.path.append('/kaggle/input/timm-pytorch-image-models/pytorch-image-models-master')\n","sys.path.append('/kaggle/input/segmentation-models-pytorch/segmentation_models.pytorch-master')\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import os\n","import shutil\n","import warnings\n","from glob import glob\n","import gc\n","\n","import cv2\n","import PIL.Image as Image\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","from scipy.ndimage import gaussian_filter\n","from scipy import ndimage\n","import torch\n","import torch.nn as nn\n","import segmentation_models_pytorch as smp\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","from torch.utils.data import DataLoader, Dataset\n","from tqdm import tqdm\n","\n","warnings.simplefilter(\"ignore\")\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## config"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2023-04-04T04:51:23.625553Z","iopub.status.busy":"2023-04-04T04:51:23.62525Z","iopub.status.idle":"2023-04-04T04:51:23.638088Z","shell.execute_reply":"2023-04-04T04:51:23.636525Z","shell.execute_reply.started":"2023-04-04T04:51:23.625523Z"},"trusted":true},"outputs":[],"source":["class CFG:\n","    # ============== comp exp name =============\n","    comp_name = 'vesuvius'\n","    comp_dir_path = '/kaggle/input/'\n","    comp_folder_name = 'vesuvius-challenge-ink-detection'\n","\n","    model_input_name = \"unet-efficientnetb0-nonflatten\"\n","    model_path = f'/kaggle/input/{model_input_name}/{model_input_name}.pth'\n","    TH = 0.45\n","    fold = 1\n","    \n","    # ============== dataset =============\n","    prefix = \"nonflatten\" # \"flatten\" \"non_flatten\"\n","    start = 29 \n","    stop = 34\n","    input_dirs = f\"/kaggle/input/vesuvius-challenge-ink-detection/test/\"\n","    stack_dir = f\"/kaggle/working/dataset_inference/\"\n","    save_dir = f\"/kaggle/working/dataset_inference/{prefix}/\"\n","    comp_dataset_path = f\"/kaggle/working/dataset_inference/{prefix}/{start}-{stop}/\"\n","\n","    # ============== pred target =============\n","    target_size = 1\n","\n","    # ============== model cfg =============\n","    model_name = 'Unet'\n","    backbone = 'efficientnet-b0'\n","    # backbone = 'se_resnext50_32x4d'\n","    in_chans = 6\n","\n","    # ============== training cfg =============\n","    size = 224\n","    tile_size = 224\n","    stride = tile_size // 4\n","\n","    batch_size = 16  # 32\n","    use_amp = True\n","\n","    # ============== fixed =============\n","    num_workers = 4\n","\n","    # ============== augmentation =============\n","    train_aug_list = [\n","        # A.RandomResizedCrop(\n","        #     size, size, scale=(0.85, 1.0)),\n","        A.Resize(size, size),\n","        A.HorizontalFlip(p=0.5),\n","        A.VerticalFlip(p=0.5),\n","        A.RandomBrightnessContrast(p=0.75),\n","        A.ShiftScaleRotate(p=0.75),\n","        A.OneOf([\n","                A.GaussNoise(var_limit=[10, 50]),\n","                A.GaussianBlur(),\n","                A.MotionBlur(),\n","                ], p=0.4),\n","        A.GridDistortion(num_steps=5, distort_limit=0.3, p=0.5),\n","        A.CoarseDropout(max_holes=1, max_width=int(size * 0.3), max_height=int(size * 0.3),\n","                        mask_fill_value=0, p=0.5),\n","        # A.Cutout(max_h_size=int(size * 0.6),\n","        #          max_w_size=int(size * 0.6), num_holes=1, p=1.0),\n","        A.Normalize(\n","            mean=[0] * in_chans,\n","            std=[1] * in_chans\n","        ),\n","        ToTensorV2(transpose_mask=True),\n","    ]\n","\n","    valid_aug_list = [\n","        A.Resize(size, size),\n","        A.Normalize(\n","            mean=[0] * in_chans,\n","            std=[1] * in_chans\n","        ),\n","        ToTensorV2(transpose_mask=True),\n","    ]\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## preprocess"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":["GAUSSIAN_BLUR_TOPOGRAPHIC_MAP = False\n","from glob import glob\n","import cv2\n","import os\n","import gc\n","import shutil\n","from scipy.ndimage import gaussian_filter\n","from scipy import ndimage\n","import numpy as np\n","import PIL.Image as Image\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","from os import path\n","\n","GAUSSIAN_BLUR_TOPOGRAPHIC_MAP = False\n","\n","\n","def split_label_mask_train(input_dir, save_dir, split):\n","    # 1~64のsurface_volumeをstackし、npy（float32）で保存\n","    # fragmentサイズが大きい場合はsplit分割\n","    fragment_i = input_dir.split(\"/\")[-1]\n","    for split_i in range(split):\n","        mask=cv2.imread(f\"{input_dir}/mask.png\", 0)\n","        label=cv2.imread(f\"{input_dir}/inklabels.png\", 0)\n","        image_height = mask.shape[0]\n","        split_height = image_height // split\n","        if split_i == split - 1:\n","            mask = mask[split_i*split_height:image_height, :]\n","            label = label[split_i*split_height:image_height, :]\n","        else:\n","            mask = mask[split_i*split_height:(split_i+1)*split_height, :]\n","            label = label[split_i*split_height:(split_i+1)*split_height, :]\n","        cv2.imwrite(save_dir + f\"mask_{fragment_i}_{split_i}.png\", mask)\n","        cv2.imwrite(save_dir + f\"inklabels_{fragment_i}_{split_i}.png\", label)\n","\n","\n","def split_label_mask_inference(input_dir, save_dir, fragment_i):\n","    shutil.copy(f\"{input_dir}/mask.png\", save_dir + f\"mask_{fragment_i}.png\")\n","\n","\n","def split_stack_image(input_dir, save_dir, split=1):\n","    fragment_i = input_dir.split(\"/\")[-1]\n","    image_height = cv2.imread(f\"{input_dir}/mask.png\", -1).shape[0]\n","    split_height = image_height // split\n","\n","    for split_i in range(split):\n","        image_stack = None\n","        images = []\n","\n","        surfaces_path = sorted(glob(f\"{input_dir}/surface_volume/*.tif\"))\n","        save_npy_path = save_dir + f\"image_stack_{fragment_i}_{split_i}.npy\"\n","        if os.path.exists(save_npy_path):\n","            continue\n","\n","        for surface_path in tqdm(surfaces_path):\n","            image = cv2.imread(surface_path, 0)\n","            if split_i < split - 1:\n","                image = image[split_i*split_height:(split_i+1)*split_height, :]\n","            else:\n","                image = image[split_i*split_height:image_height, :]\n","            images.append(image)\n","            del image\n","        image_stack = np.stack(images)\n","\n","        del images\n","        gc.collect()\n","\n","        image_stack = (image_stack).astype(np.uint8)\n","        with open(save_npy_path, 'wb') as f:\n","            np.save(f, image_stack, allow_pickle=True)\n","        del image_stack\n","        gc.collect()\n","\n","\n","def flatten(image_stack, x, y, range, z_buffer):\n","    clipped_stack = image_stack[:, x:x+range, y:y+range]  # smaller portion\n","    clipped_stack=(clipped_stack/255)\n","    clipped_stack=np.flip(clipped_stack,axis=0)\n","    gauss_stack = gaussian_filter(clipped_stack, sigma=1)  # blur data a little bit\n","    gauss_stack = ndimage.sobel(gauss_stack, axis=0)  # detect edges in top-down direction\n","    gauss_stack = gaussian_filter(gauss_stack, sigma=1)  # blur again\n","\n","    filtered_stack = np.where(gauss_stack >= 0.5, 1, 0)  # type: ignore\n","    topographic_map = np.argmax(filtered_stack, axis=0)\n","    if GAUSSIAN_BLUR_TOPOGRAPHIC_MAP:\n","        topographic_map = gaussian_filter(topographic_map, sigma=1)\n","\n","    is_idx = np.indices(clipped_stack.shape)\n","    flatten_stack = clipped_stack[(is_idx[0] + topographic_map-z_buffer) % clipped_stack.shape[0], is_idx[1], is_idx[2]]\n","    flatten_stack=(np.flip(flatten_stack,axis=0)*255).astype(\"uint8\")\n","\n","    # return clipped_stack, gauss_stack, filtered_stack, topographic_map, flatten_stack\n","    return flatten_stack\n","\n","\n","def whole_flatten(dataset_dir, image_stack_dir, fragment_i, split_i, delete=False):\n","    save_path = os.path.join(image_stack_dir + f\"flatten_stack_{fragment_i}_{split_i}.npy\")\n","    image_stack_path = dataset_dir + f\"image_stack_{fragment_i}_{split_i}.npy\"\n","\n","    if os.path.exists(save_path):\n","        return\n","\n","    image_stack = np.load(open(image_stack_path, 'rb'))\n","\n","    _, image_stack_x, image_stack_y = image_stack.shape\n","    flatten_array = np.zeros_like(image_stack)\n","\n","    for x in range(0, image_stack_x, 500):\n","        for y in range(0, image_stack_y, 500):\n","            flattened_stack = flatten(image_stack, x, y, 500, 5)\n","            flatten_array[:, x:x+500, y:y+500] = flattened_stack\n","            del flattened_stack\n","    with open(save_path, 'wb') as f:\n","        np.save(f, flatten_array, allow_pickle=True)\n","\n","    del flatten_array\n","    gc.collect()\n","\n","    if delete:\n","        os.remove(os.path.join(image_stack_path))\n","\n","\n","def extract_flatten_layers(input_dir, save_dir, fragment_i, split_i, start, stop, delete=False):\n","    input_stack_path = f\"{input_dir}/flatten_stack_{fragment_i}_{split_i}.npy\"\n","    output_stack_path=f\"{save_dir}/{fragment_i}_{split_i}.npy\"\n","\n","    stack = np.load(open(input_stack_path, 'rb'))\n","    stack = stack[-stop-1:-start, :, :]\n","\n","    with open(output_stack_path, 'wb') as f:\n","        np.save(f, stack, allow_pickle=True)\n","    del stack\n","    gc.collect()\n","\n","    if delete:\n","        os.remove(os.path.join(input_stack_path))\n","\n","def extract_nonflatten_layers(input_dir, save_dir, fragment_i, split_i, start, stop, delete=False):\n","    input_stack_path = f\"{input_dir}/image_stack_{fragment_i}_{split_i}.npy\"\n","    output_stack_path=f\"{save_dir}/{fragment_i}_{split_i}.npy\"\n","\n","    stack = np.load(open(input_stack_path, 'rb'))\n","    stack = stack[start:stop+1, :, :]\n","\n","    with open(output_stack_path, 'wb') as f:\n","        np.save(f, stack, allow_pickle=True)\n","    del stack\n","    gc.collect()\n","\n","    if delete:\n","        os.remove(os.path.join(input_stack_path))\n","\n","\n","def concat_npy(save_dir, start, stop, fragment_i, delete=False):\n","    npy_list = []\n","    for npy in sorted(glob(f\"{save_dir}/{fragment_i}_*.npy\")):\n","        print(npy)\n","        npy_list.append(np.load(open(npy, 'rb')))\n","        if delete:\n","            os.remove(npy)\n","    result = np.concatenate(npy_list, axis=1)\n","    output_stack_fname = f\"{fragment_i}.npy\"\n","    with open(f\"{save_dir}/{output_stack_fname}\", 'wb') as f:\n","        np.save(f, result, allow_pickle=True)\n","\n","\n","def dataset_preprocess_nonflatten(input_dir, dataset_dir, split, start, stop, train=True, delete=True):\n","    image_stack_dir = f\"{dataset_dir}/nonflatten/\"\n","    extract_save_dir = f\"{image_stack_dir}/{start}-{stop}/\"\n","    os.makedirs(dataset_dir, exist_ok=True)\n","    os.makedirs(image_stack_dir, exist_ok=True)\n","    os.makedirs(extract_save_dir, exist_ok=True)\n","\n","    fragment_i = input_dir.split(\"/\")[-1]\n","    print(input_dir)\n","    if train:\n","        split_label_mask_train(input_dir, dataset_dir, split)\n","    else:\n","        split_label_mask_inference(input_dir, dataset_dir, fragment_i)\n","\n","    split_stack_image(input_dir, dataset_dir, split)\n","\n","    for split_i in range(split):\n","        extract_nonflatten_layers(dataset_dir, extract_save_dir, fragment_i, split_i, start, stop, delete)\n","    if not train:\n","        print(extract_save_dir)\n","        concat_npy(extract_save_dir, start, stop, fragment_i, delete)\n","\n","\n","\n","def dataset_preprocess_flatten(input_dir, dataset_dir, split, start, stop, train=True, delete=True):\n","    image_stack_dir = f\"{dataset_dir}/flatten/\"\n","    extract_save_dir = f\"{image_stack_dir}/{start}-{stop}/\"\n","    os.makedirs(dataset_dir, exist_ok=True)\n","    os.makedirs(image_stack_dir, exist_ok=True)\n","    os.makedirs(extract_save_dir, exist_ok=True)\n","\n","    fragment_i = input_dir.split(\"/\")[-1]\n","    print(input_dir)\n","    if train:\n","        split_label_mask_train(input_dir, dataset_dir, split)\n","    else:\n","        split_label_mask_inference(input_dir, dataset_dir, fragment_i)\n","\n","    split_stack_image(input_dir, dataset_dir, split)\n","\n","    for split_i in range(split):\n","        whole_flatten(dataset_dir, image_stack_dir, fragment_i, split_i, delete=False)\n","        extract_flatten_layers(image_stack_dir, extract_save_dir, fragment_i, split_i, start, stop, delete)\n","\n","    if not train:\n","        print(extract_save_dir)\n","        concat_npy(extract_save_dir, start, stop, fragment_i, delete)\n"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 65/65 [00:51<00:00,  1.26it/s]\n"]},{"name":"stdout","output_type":"stream","text":["/kaggle/working/dataset_inference/non_flatten//29-34/a_0.npy\n"]},{"name":"stderr","output_type":"stream","text":[" 63%|██████▎   | 41/65 [01:02<00:36,  1.52s/it]\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[5], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mCFG\u001b[39m.\u001b[39mcomp_dataset_path\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mfragment_i\u001b[39m}\u001b[39;00m\u001b[39m.npy\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     18\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m stack_image(input_dirs, stack_dir, fragment_i, split)\n\u001b[1;32m     21\u001b[0m \u001b[39mfor\u001b[39;00m split_i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(split):\n\u001b[1;32m     22\u001b[0m     split_label_mask(input_dirs, stack_dir, fragment_i, split, split_i, train\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n","Cell \u001b[0;32mIn[4], line 38\u001b[0m, in \u001b[0;36mstack_image\u001b[0;34m(input_dir, save_dir, fragment_i, split)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[39mfor\u001b[39;00m surface_path \u001b[39min\u001b[39;00m tqdm(surfaces_path):\n\u001b[0;32m---> 38\u001b[0m     image \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39;49mimread(surface_path, \u001b[39m0\u001b[39;49m)\n\u001b[1;32m     39\u001b[0m     \u001b[39mif\u001b[39;00m split_i \u001b[39m<\u001b[39m split \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m     40\u001b[0m         image \u001b[39m=\u001b[39m image[split_i\u001b[39m*\u001b[39msplit_height:(split_i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m*\u001b[39msplit_height, :]\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["#inference\n","input_dirs=\"/kaggle/input/vesuvius-challenge-ink-detection/test/\"\n","dataset_dir = f\"/kaggle/working/dataset_inference/\"\n","split=1\n","delete=False\n","\n","for input_dir in glob(f\"{input_dirs}/*\"):\n","    dataset_preprocess_nonflatten(f\"{input_dir}\", dataset_dir, split, 29, 34, train=False, delete=delete)\n","    # dataset_preprocess_flatten(f\"{input_dir}\", dataset_dir, split, 3, 8, train=False, delete=delete)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## helper"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2023-04-04T04:51:23.813569Z","iopub.status.busy":"2023-04-04T04:51:23.813141Z","iopub.status.idle":"2023-04-04T04:51:23.824293Z","shell.execute_reply":"2023-04-04T04:51:23.823167Z","shell.execute_reply.started":"2023-04-04T04:51:23.813471Z"},"trusted":true},"outputs":[],"source":["# ref.: https://www.kaggle.com/stainsby/fast-tested-rle\n","def rle(img):\n","    '''\n","    img: numpy array, 1 - mask, 0 - background\n","    Returns run length as string formated\n","    '''\n","    pixels = img.flatten()\n","    # pixels = (pixels >= thr).astype(int)\n","\n","    pixels = np.concatenate([[0], pixels, [0]])\n","    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n","    runs[1::2] -= runs[::2]\n","    return ' '.join(str(x) for x in runs)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## dataset"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[],"source":["def preprocess(image):\n","    # image=np.clip(image, a_min=0.15,a_max=0.7)\n","    return image\n"]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2023-04-04T04:51:23.826492Z","iopub.status.busy":"2023-04-04T04:51:23.826048Z","iopub.status.idle":"2023-04-04T04:51:23.835717Z","shell.execute_reply":"2023-04-04T04:51:23.834666Z","shell.execute_reply.started":"2023-04-04T04:51:23.826453Z"},"trusted":true},"outputs":[],"source":["def read_image(fragment_id):\n","    image_stack = np.load(open(f\"{CFG.comp_dataset_path}/{fragment_id}.npy\", 'rb'))\n","\n","    pad0 = (CFG.tile_size - image_stack.shape[1] % CFG.tile_size)\n","    pad1 = (CFG.tile_size - image_stack.shape[2] % CFG.tile_size)\n","\n","    image_stack = np.pad(image_stack, [(0, 0), (0, pad0), (0, pad1)], constant_values=0)\n","    image_stack = image_stack.transpose((1, 2, 0))\n","    image_stack = preprocess(image_stack)\n","\n","    return image_stack\n","\n","def get_transforms(data, cfg):\n","    if data == 'train':\n","        aug = A.Compose(cfg.train_aug_list)\n","    elif data == 'valid':\n","        aug = A.Compose(cfg.valid_aug_list)\n","\n","    # print(aug)\n","    return aug\n","\n","\n","class CustomDataset(Dataset):\n","    def __init__(self, images, cfg, labels=None, transform=None):\n","        self.images = images\n","        self.cfg = cfg\n","        self.labels = labels\n","        self.transform = transform\n","\n","    def __len__(self):\n","        # return len(self.xyxys)\n","        return len(self.images)\n","\n","    def __getitem__(self, idx):\n","        # x1, y1, x2, y2 = self.xyxys[idx]\n","        image = self.images[idx]\n","        data = self.transform(image=image)\n","        image = data['image']\n","        return image\n","\n","def make_test_dataset(fragment_id):\n","    test_images = read_image(fragment_id)\n","    x1_list = list(range(0, test_images.shape[1]-CFG.tile_size+1, CFG.stride))\n","    y1_list = list(range(0, test_images.shape[0]-CFG.tile_size+1, CFG.stride))\n","\n","    test_images_list = []\n","    xyxys = []\n","    for y1 in y1_list:\n","        for x1 in x1_list:\n","            y2 = y1 + CFG.tile_size\n","            x2 = x1 + CFG.tile_size\n","\n","            test_images_list.append(test_images[y1:y2, x1:x2])\n","            xyxys.append((x1, y1, x2, y2))\n","    # xyxys = np.stack(xyxys)\n","\n","    test_dataset = CustomDataset(test_images_list, CFG, transform=get_transforms(data='valid', cfg=CFG))\n","\n","    test_loader = DataLoader(test_dataset,\n","                             batch_size=CFG.batch_size,\n","                             shuffle=False,\n","                             num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n","    # return test_images_list\n","    return test_loader, xyxys\n"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["uint8\n","(4704, 6496, 6)\n","255\n"]},{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["image = read_image(\"a\")\n","print(image.dtype)\n","print(image.shape)\n","print(image.max())\n","plt.hist(image.flatten(), bins=100);\n"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[],"source":["for i, image in enumerate((make_test_dataset(\"a\")[0])):\n","    if i != 27:\n","        continue\n","    print(image.shape)\n","    plt.figure()\n","    plt.imshow(image[0, 0, :, :])\n","    plt.figure()\n","    plt.hist(image[0, 0, :, :].flatten(), bins=100)\n","    break\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## model"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-04-04T04:51:23.865123Z","iopub.status.busy":"2023-04-04T04:51:23.864743Z","iopub.status.idle":"2023-04-04T04:51:23.87814Z","shell.execute_reply":"2023-04-04T04:51:23.877181Z","shell.execute_reply.started":"2023-04-04T04:51:23.86506Z"},"trusted":true},"outputs":[],"source":["class CustomModel(nn.Module):\n","    def __init__(self, cfg, weight=None):\n","        super().__init__()\n","        self.cfg = cfg\n","\n","        self.encoder = smp.Unet(\n","            encoder_name=cfg.backbone,\n","            encoder_weights=weight,\n","            in_channels=cfg.in_chans,\n","            classes=cfg.target_size,\n","            activation=None,\n","        )\n","\n","    def forward(self, image):\n","        output = self.encoder(image)\n","        output = output.squeeze(-1)\n","        return output\n","\n","\n","def build_model(cfg, weight=\"imagenet\"):\n","    print('model_name', cfg.model_name)\n","    print('backbone', cfg.backbone)\n","\n","    model = CustomModel(cfg, weight)\n","    return model\n"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-04-04T04:51:23.881751Z","iopub.status.busy":"2023-04-04T04:51:23.88138Z","iopub.status.idle":"2023-04-04T04:51:23.891133Z","shell.execute_reply":"2023-04-04T04:51:23.890126Z","shell.execute_reply.started":"2023-04-04T04:51:23.881723Z"},"trusted":true},"outputs":[],"source":["class EnsembleModel:\n","    def __init__(self, use_tta=False):\n","        self.models = []\n","        self.use_tta = use_tta\n","\n","    def __call__(self, x):\n","        outputs = [torch.sigmoid(model(x)).to('cpu').numpy()\n","                   for model in self.models]\n","        avg_preds = np.mean(outputs, axis=0)\n","        return avg_preds\n","\n","    def add_model(self, model):\n","        self.models.append(model)\n","\n","\n","def build_ensemble_model():\n","    model = EnsembleModel()\n","    for fold in range(CFG.fold):\n","        _model = build_model(CFG, weight=None)\n","        _model.to(device)\n","\n","        # model_path = f'/kaggle/input/vesuvius-models-public/vesuvius_2d_slide_exp002/vesuvius-models/Unet_fold{fold}_best.pth'\n","        model_path = CFG.model_path\n","        print(model_path+\"\\n\")\n","\n","        state = torch.load(model_path)['model']\n","        _model.load_state_dict(state)\n","        _model.eval()\n","\n","        model.add_model(_model)\n","\n","    return model\n"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-04-04T04:51:23.892968Z","iopub.status.busy":"2023-04-04T04:51:23.892497Z","iopub.status.idle":"2023-04-04T04:51:23.908Z","shell.execute_reply":"2023-04-04T04:51:23.906772Z","shell.execute_reply.started":"2023-04-04T04:51:23.892927Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["model_name Unet\n","backbone efficientnet-b0\n","/kaggle/input/unet-efficientnetb0-nonflatten/unet-efficientnetb0-nonflatten.pth\n","\n"]}],"source":["fragment_ids = sorted(os.listdir(CFG.input_dirs))\n","\n","model = build_ensemble_model()\n","\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## main"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-04-04T04:52:01.418536Z","iopub.status.busy":"2023-04-04T04:52:01.417925Z","iopub.status.idle":"2023-04-04T05:04:59.789188Z","shell.execute_reply":"2023-04-04T05:04:59.788009Z","shell.execute_reply.started":"2023-04-04T04:52:01.418493Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":[" 48%|████▊     | 168/347 [00:03<00:04, 44.06it/s]\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[14], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m batch_size \u001b[39m=\u001b[39m images\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)\n\u001b[1;32m     25\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> 26\u001b[0m     y_preds \u001b[39m=\u001b[39m model(images)\n\u001b[1;32m     28\u001b[0m start_idx \u001b[39m=\u001b[39m step\u001b[39m*\u001b[39mCFG\u001b[39m.\u001b[39mbatch_size\n\u001b[1;32m     29\u001b[0m end_idx \u001b[39m=\u001b[39m start_idx \u001b[39m+\u001b[39m batch_size\n","Cell \u001b[0;32mIn[12], line 7\u001b[0m, in \u001b[0;36mEnsembleModel.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m----> 7\u001b[0m     outputs \u001b[39m=\u001b[39m [torch\u001b[39m.\u001b[39msigmoid(model(x))\u001b[39m.\u001b[39mto(\u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m      8\u001b[0m                \u001b[39mfor\u001b[39;00m model \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodels]\n\u001b[1;32m      9\u001b[0m     avg_preds \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean(outputs, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m     10\u001b[0m     \u001b[39mreturn\u001b[39;00m avg_preds\n","Cell \u001b[0;32mIn[12], line 7\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m----> 7\u001b[0m     outputs \u001b[39m=\u001b[39m [torch\u001b[39m.\u001b[39;49msigmoid(model(x))\u001b[39m.\u001b[39;49mto(\u001b[39m'\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m      8\u001b[0m                \u001b[39mfor\u001b[39;00m model \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodels]\n\u001b[1;32m      9\u001b[0m     avg_preds \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean(outputs, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m     10\u001b[0m     \u001b[39mreturn\u001b[39;00m avg_preds\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["results = []\n","for fragment_id in fragment_ids:\n","\n","    test_loader, xyxys = make_test_dataset(fragment_id)\n","\n","    binary_mask = cv2.imread(CFG.input_dirs + f\"{fragment_id}/mask.png\", 0)\n","    binary_mask = (binary_mask / 255).astype(int)\n","\n","    ori_h = binary_mask.shape[0]\n","    ori_w = binary_mask.shape[1]\n","    # mask = mask / 255\n","\n","    pad0 = (CFG.tile_size - binary_mask.shape[0] % CFG.tile_size)\n","    pad1 = (CFG.tile_size - binary_mask.shape[1] % CFG.tile_size)\n","\n","    binary_mask = np.pad(binary_mask, [(0, pad0), (0, pad1)], constant_values=0)\n","\n","    mask_pred = np.zeros(binary_mask.shape)\n","    mask_count = np.zeros(binary_mask.shape)\n","\n","    for step, (images) in tqdm(enumerate(test_loader), total=len(test_loader)):\n","        images = images.to(device)\n","        batch_size = images.size(0)\n","\n","        with torch.no_grad():\n","            y_preds = model(images)\n","\n","        start_idx = step*CFG.batch_size\n","        end_idx = start_idx + batch_size\n","        for i, (x1, y1, x2, y2) in enumerate(xyxys[start_idx:end_idx]):\n","            mask_pred[y1:y2, x1:x2] += y_preds[i].squeeze(0)\n","            mask_count[y1:y2, x1:x2] += np.ones((CFG.tile_size, CFG.tile_size))\n","\n","    print(f'mask_count_min: {mask_count.min()}')\n","    mask_pred /= mask_count\n","\n","    mask_pred = mask_pred[:ori_h, :ori_w]\n","    binary_mask = binary_mask[:ori_h, :ori_w]\n","\n","    mask_pred = (mask_pred >= CFG.TH).astype(int)\n","    mask_pred *= binary_mask\n","\n","    plt.imshow(mask_pred)\n","    plt.show()\n","\n","    inklabels_rle = rle(mask_pred)\n","\n","    results.append((fragment_id, inklabels_rle))\n","\n","    # del mask_pred, mask_count\n","    # del test_loader\n","\n","    gc.collect()\n","    torch.cuda.empty_cache()\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## submission"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2023-04-04T05:04:59.79253Z","iopub.status.busy":"2023-04-04T05:04:59.791024Z","iopub.status.idle":"2023-04-04T05:04:59.80219Z","shell.execute_reply":"2023-04-04T05:04:59.801265Z","shell.execute_reply.started":"2023-04-04T05:04:59.792484Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Id</th>\n","      <th>Predicted</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>a</td>\n","      <td>561955 4 568280 18 574603 26 580926 33 580964 ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>b</td>\n","      <td>1284 189 1753 1 1757 136 2048 21 2076 46 2129 ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  Id                                          Predicted\n","0  a  561955 4 568280 18 574603 26 580926 33 580964 ...\n","1  b  1284 189 1753 1 1757 136 2048 21 2076 46 2129 ..."]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["sub = pd.DataFrame(results, columns=['Id', 'Predicted'])\n","sample_sub = pd.read_csv(\"/kaggle/input/vesuvius-challenge-ink-detection/sample_submission.csv\")\n","sample_sub = pd.merge(sample_sub[['Id']], sub, on='Id', how='left')\n","sample_sub.to_csv(\"submission.csv\", index=False)\n","sample_sub\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"}},"nbformat":4,"nbformat_minor":4}
